{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smartsheet Agent Evaluation\n",
    "\n",
    "This notebook evaluates the Smartsheet Agent across multiple dimensions:\n",
    "\n",
    "1. **Tool Reliability** - Does the agent call the correct tools for queries?\n",
    "2. **Response Accuracy** - Are responses helpful and accurate?\n",
    "3. **Constraint Adherence** - Does the agent respect read-only limitations?\n",
    "\n",
    "Uses LangWatch Evaluations API for tracking and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(project_root / \".env\")\n",
    "\n",
    "import langwatch\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize LangWatch\n",
    "langwatch.setup()\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"LangWatch configured: {bool(os.getenv('LANGWATCH_API_KEY'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "dataset_path = Path.cwd() / \"smartsheet_test_dataset.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "print(f\"Loaded {len(df)} test cases\")\n",
    "print(f\"\\nCategories: {df['category'].value_counts().to_dict()}\")\n",
    "print(f\"\\nDifficulty: {df['difficulty'].value_counts().to_dict()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agno.agent import Agent\n",
    "from agno.models.openrouter import OpenRouter\n",
    "\n",
    "from smartsheet_tools import SMARTSHEET_TOOLS\n",
    "\n",
    "# Get system prompt from LangWatch\n",
    "prompt = langwatch.prompts.get(\"smartsheet-agent\")\n",
    "system_prompt = \"\"\n",
    "for message in prompt.messages:\n",
    "    if message.get(\"role\") == \"system\":\n",
    "        system_prompt = message.get(\"content\", \"\")\n",
    "        break\n",
    "\n",
    "def create_test_agent():\n",
    "    \"\"\"Create a fresh agent for testing.\"\"\"\n",
    "    return Agent(\n",
    "        name=\"Smartsheet Agent\",\n",
    "        model=OpenRouter(id=os.getenv(\"OPENROUTER_MODEL\", \"google/gemini-2.5-flash\")),\n",
    "        tools=SMARTSHEET_TOOLS,\n",
    "        instructions=system_prompt,\n",
    "        markdown=True,\n",
    "    )\n",
    "\n",
    "# Test agent creation\n",
    "test_agent = create_test_agent()\n",
    "print(f\"Agent created with {len(SMARTSHEET_TOOLS)} tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation 1: Tool Reliability\n",
    "\n",
    "This evaluation checks if the agent calls the expected tools for each query type.\n",
    "We track:\n",
    "- Whether the correct tool was called\n",
    "- Tool call accuracy per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tool_calls(response):\n",
    "    \"\"\"Extract tool names from agent response.\"\"\"\n",
    "    tool_calls = []\n",
    "\n",
    "    # Check for tool calls in the response\n",
    "    if hasattr(response, 'messages'):\n",
    "        for msg in response.messages:\n",
    "            if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                for tc in msg.tool_calls:\n",
    "                    if hasattr(tc, 'function'):\n",
    "                        tool_calls.append(tc.function.name)\n",
    "                    elif hasattr(tc, 'name'):\n",
    "                        tool_calls.append(tc.name)\n",
    "\n",
    "    # Also check in run_response if available\n",
    "    if hasattr(response, 'run_response'):\n",
    "        rr = response.run_response\n",
    "        if hasattr(rr, 'tool_calls') and rr.tool_calls:\n",
    "            for tc in rr.tool_calls:\n",
    "                if hasattr(tc, 'name'):\n",
    "                    tool_calls.append(tc.name)\n",
    "\n",
    "    return list(set(tool_calls))  # Remove duplicates\n",
    "\n",
    "\n",
    "def check_tool_reliability(query: str, expected_tool: str, agent: Agent) -> dict:\n",
    "    \"\"\"\n",
    "    Run a query and check if the expected tool was called.\n",
    "\n",
    "    Returns dict with:\n",
    "    - passed: bool\n",
    "    - expected_tool: str\n",
    "    - actual_tools: list\n",
    "    - response_content: str\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = agent.run(query)\n",
    "\n",
    "        # Get tool calls from response\n",
    "        actual_tools = extract_tool_calls(response)\n",
    "\n",
    "        # For NONE expected, check that response mentions read-only\n",
    "        if expected_tool == \"NONE\":\n",
    "            content = response.content.lower() if response.content else \"\"\n",
    "            passed = any(kw in content for kw in [\"read-only\", \"cannot\", \"can't\", \"unable\", \"don't have\"])\n",
    "        else:\n",
    "            passed = expected_tool in actual_tools\n",
    "\n",
    "        return {\n",
    "            \"passed\": passed,\n",
    "            \"expected_tool\": expected_tool,\n",
    "            \"actual_tools\": actual_tools,\n",
    "            \"response_content\": response.content[:500] if response.content else \"\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"passed\": False,\n",
    "            \"expected_tool\": expected_tool,\n",
    "            \"actual_tools\": [],\n",
    "            \"response_content\": f\"Error: {str(e)}\"\n",
    "        }\n",
    "\n",
    "print(\"Tool reliability checker ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Tool Reliability Evaluation\n",
    "# Filter to a subset for faster testing (adjust as needed)\n",
    "test_df = df.sample(n=min(10, len(df)), random_state=42)  # Sample 10 for quick testing\n",
    "\n",
    "evaluation = langwatch.evaluation.init(\"smartsheet-tool-reliability\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, row in evaluation.loop(test_df.iterrows()):\n",
    "    def evaluate(idx, row):\n",
    "        agent = create_test_agent()\n",
    "\n",
    "        result = check_tool_reliability(\n",
    "            query=row[\"query\"],\n",
    "            expected_tool=row[\"expected_tool\"],\n",
    "            agent=agent\n",
    "        )\n",
    "\n",
    "        # Log to LangWatch\n",
    "        evaluation.log(\n",
    "            \"tool_called_correctly\",\n",
    "            index=idx,\n",
    "            passed=result[\"passed\"],\n",
    "            data={\n",
    "                \"query\": row[\"query\"],\n",
    "                \"expected_tool\": row[\"expected_tool\"],\n",
    "                \"actual_tools\": result[\"actual_tools\"],\n",
    "                \"category\": row[\"category\"],\n",
    "                \"response_preview\": result[\"response_content\"][:200]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"query\": row[\"query\"],\n",
    "            \"category\": row[\"category\"],\n",
    "            **result\n",
    "        })\n",
    "\n",
    "        print(f\"{'✓' if result['passed'] else '✗'} {row['query'][:50]}... -> {result['actual_tools']}\")\n",
    "\n",
    "    evaluation.submit(evaluate, idx, row)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Tool Reliability Results: {sum(r['passed'] for r in results)}/{len(results)} passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation 2: Response Accuracy\n",
    "\n",
    "This evaluation uses LLM-as-judge to assess response quality:\n",
    "- Helpfulness\n",
    "- Relevance to query\n",
    "- Appropriate use of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client for LLM-as-judge\n",
    "judge_client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
    ")\n",
    "\n",
    "def judge_response_quality(query: str, response: str, expected_keywords: str) -> dict:\n",
    "    \"\"\"\n",
    "    Use LLM-as-judge to evaluate response quality.\n",
    "\n",
    "    Returns:\n",
    "    - score: 0-10\n",
    "    - reasoning: explanation\n",
    "    - passed: score >= 7\n",
    "    \"\"\"\n",
    "    judge_prompt = f\"\"\"You are evaluating an AI assistant's response to a user query about Smartsheet data.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Response: {response}\n",
    "\n",
    "Expected keywords/concepts: {expected_keywords}\n",
    "\n",
    "Rate the response on a scale of 0-10 based on:\n",
    "1. Relevance: Does it address the user's query?\n",
    "2. Helpfulness: Does it provide useful information or guidance?\n",
    "3. Accuracy: Does it use correct terminology and concepts?\n",
    "4. Completeness: Does it cover the key aspects mentioned?\n",
    "\n",
    "Respond in this exact format:\n",
    "SCORE: [0-10]\n",
    "REASONING: [brief explanation]\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        result = judge_client.chat.completions.create(\n",
    "            model=\"openai/gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
    "            max_tokens=200\n",
    "        )\n",
    "\n",
    "        judge_response = result.choices[0].message.content\n",
    "\n",
    "        # Parse score\n",
    "        import re\n",
    "        score_match = re.search(r'SCORE:\\s*(\\d+)', judge_response)\n",
    "        score = int(score_match.group(1)) if score_match else 5\n",
    "\n",
    "        # Parse reasoning\n",
    "        reasoning_match = re.search(r'REASONING:\\s*(.+)', judge_response, re.DOTALL)\n",
    "        reasoning = reasoning_match.group(1).strip() if reasoning_match else \"No reasoning provided\"\n",
    "\n",
    "        return {\n",
    "            \"score\": score,\n",
    "            \"reasoning\": reasoning,\n",
    "            \"passed\": score >= 7\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"score\": 0,\n",
    "            \"reasoning\": f\"Judge error: {str(e)}\",\n",
    "            \"passed\": False\n",
    "        }\n",
    "\n",
    "print(\"Response quality judge ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Response Accuracy Evaluation\n",
    "# Use same test subset\n",
    "accuracy_evaluation = langwatch.evaluation.init(\"smartsheet-response-accuracy\")\n",
    "\n",
    "accuracy_results = []\n",
    "\n",
    "for idx, row in accuracy_evaluation.loop(test_df.iterrows()):\n",
    "    def evaluate_accuracy(idx, row):\n",
    "        agent = create_test_agent()\n",
    "\n",
    "        try:\n",
    "            response = agent.run(row[\"query\"])\n",
    "            response_content = response.content if response.content else \"No response\"\n",
    "        except Exception as e:\n",
    "            response_content = f\"Error: {str(e)}\"\n",
    "\n",
    "        # Judge the response\n",
    "        judge_result = judge_response_quality(\n",
    "            query=row[\"query\"],\n",
    "            response=response_content,\n",
    "            expected_keywords=row[\"expected_keywords\"]\n",
    "        )\n",
    "\n",
    "        # Log to LangWatch\n",
    "        accuracy_evaluation.log(\n",
    "            \"response_quality\",\n",
    "            index=idx,\n",
    "            score=judge_result[\"score\"] / 10.0,  # Normalize to 0-1\n",
    "            data={\n",
    "                \"query\": row[\"query\"],\n",
    "                \"response_preview\": response_content[:300],\n",
    "                \"reasoning\": judge_result[\"reasoning\"],\n",
    "                \"category\": row[\"category\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        accuracy_results.append({\n",
    "            \"query\": row[\"query\"],\n",
    "            \"category\": row[\"category\"],\n",
    "            **judge_result\n",
    "        })\n",
    "\n",
    "        print(f\"Score: {judge_result['score']}/10 - {row['query'][:40]}...\")\n",
    "\n",
    "    accuracy_evaluation.submit(evaluate_accuracy, idx, row)\n",
    "\n",
    "avg_score = sum(r[\"score\"] for r in accuracy_results) / len(accuracy_results) if accuracy_results else 0\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Response Accuracy Average Score: {avg_score:.1f}/10\")\n",
    "print(f\"Passed (>=7): {sum(r['passed'] for r in accuracy_results)}/{len(accuracy_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation 3: Read-Only Constraint Adherence\n",
    "\n",
    "This evaluation specifically tests that the agent correctly refuses write operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to constraint test cases\n",
    "constraint_df = df[df[\"category\"] == \"constraint\"]\n",
    "print(f\"Testing {len(constraint_df)} constraint scenarios\")\n",
    "\n",
    "constraint_evaluation = langwatch.evaluation.init(\"smartsheet-constraint-adherence\")\n",
    "\n",
    "constraint_results = []\n",
    "\n",
    "for idx, row in constraint_evaluation.loop(constraint_df.iterrows()):\n",
    "    def evaluate_constraint(idx, row):\n",
    "        agent = create_test_agent()\n",
    "\n",
    "        try:\n",
    "            response = agent.run(row[\"query\"])\n",
    "            response_content = response.content.lower() if response.content else \"\"\n",
    "        except Exception as e:\n",
    "            response_content = f\"error: {str(e)}\"\n",
    "\n",
    "        # Check if agent correctly refuses\n",
    "        refusal_indicators = [\n",
    "            \"read-only\", \"cannot\", \"can't\", \"unable\",\n",
    "            \"don't have\", \"not able\", \"not possible\",\n",
    "            \"only read\", \"no write\", \"view only\"\n",
    "        ]\n",
    "\n",
    "        correctly_refused = any(indicator in response_content for indicator in refusal_indicators)\n",
    "\n",
    "        # Log to LangWatch\n",
    "        constraint_evaluation.log(\n",
    "            \"correctly_refused_write\",\n",
    "            index=idx,\n",
    "            passed=correctly_refused,\n",
    "            data={\n",
    "                \"query\": row[\"query\"],\n",
    "                \"response_preview\": response_content[:300]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        constraint_results.append({\n",
    "            \"query\": row[\"query\"],\n",
    "            \"passed\": correctly_refused,\n",
    "            \"response_preview\": response_content[:200]\n",
    "        })\n",
    "\n",
    "        print(f\"{'✓' if correctly_refused else '✗'} {row['query'][:50]}...\")\n",
    "\n",
    "    constraint_evaluation.submit(evaluate_constraint, idx, row)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Constraint Adherence: {sum(r['passed'] for r in constraint_results)}/{len(constraint_results)} correctly refused\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SMARTSHEET AGENT EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Tool Reliability\n",
    "tool_pass_rate = sum(r['passed'] for r in results) / len(results) * 100 if results else 0\n",
    "print(f\"\\n1. Tool Reliability: {tool_pass_rate:.1f}%\")\n",
    "print(f\"   - Correct tool called: {sum(r['passed'] for r in results)}/{len(results)}\")\n",
    "\n",
    "# Response Accuracy\n",
    "avg_score = sum(r[\"score\"] for r in accuracy_results) / len(accuracy_results) if accuracy_results else 0\n",
    "accuracy_pass_rate = sum(r['passed'] for r in accuracy_results) / len(accuracy_results) * 100 if accuracy_results else 0\n",
    "print(f\"\\n2. Response Accuracy: {avg_score:.1f}/10 average\")\n",
    "print(f\"   - Quality threshold (>=7): {accuracy_pass_rate:.1f}%\")\n",
    "\n",
    "# Constraint Adherence\n",
    "constraint_pass_rate = sum(r['passed'] for r in constraint_results) / len(constraint_results) * 100 if constraint_results else 0\n",
    "print(f\"\\n3. Constraint Adherence: {constraint_pass_rate:.1f}%\")\n",
    "print(f\"   - Correctly refused writes: {sum(r['passed'] for r in constraint_results)}/{len(constraint_results)}\")\n",
    "\n",
    "# Overall\n",
    "overall = (tool_pass_rate + accuracy_pass_rate + constraint_pass_rate) / 3\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"OVERALL SCORE: {overall:.1f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nRecommendations:\")\n",
    "if tool_pass_rate < 80:\n",
    "    print(\"- Review tool selection logic in system prompt\")\n",
    "if avg_score < 7:\n",
    "    print(\"- Improve response clarity and completeness\")\n",
    "if constraint_pass_rate < 100:\n",
    "    print(\"- Strengthen read-only messaging in system prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category-wise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze by category\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    print(\"\\nTool Reliability by Category:\")\n",
    "    print(\"-\" * 40)\n",
    "    category_stats = results_df.groupby('category')['passed'].agg(['sum', 'count'])\n",
    "    category_stats['pass_rate'] = (category_stats['sum'] / category_stats['count'] * 100).round(1)\n",
    "    print(category_stats.rename(columns={'sum': 'passed', 'count': 'total'}))\n",
    "\n",
    "if accuracy_results:\n",
    "    accuracy_df = pd.DataFrame(accuracy_results)\n",
    "\n",
    "    print(\"\\nResponse Quality by Category:\")\n",
    "    print(\"-\" * 40)\n",
    "    quality_stats = accuracy_df.groupby('category')['score'].agg(['mean', 'min', 'max']).round(1)\n",
    "    print(quality_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "View your evaluation results in the LangWatch dashboard:\n",
    "https://app.langwatch.ai/\n",
    "\n",
    "To run the full dataset, change the sample size in the evaluation cells:\n",
    "```python\n",
    "test_df = df  # Use full dataset\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
